{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdfbe1e-e9ee-4975-866c-5426663e7516",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d859af-23d1-4652-83ff-c239f53de2f9",
   "metadata": {},
   "source": [
    "- Address Overfitting (Regularization, Early Stopping)\n",
    "    - Conclusion from the Results\n",
    "        - Overview of Metrics\n",
    "        - Training Set Classification Report (old vs new)\n",
    "        - Test Set Classification Report (old vs new)\n",
    "        - Key Insights\n",
    "        - Recommendations\n",
    "- Address Underfitting (RandomizedSearchCV with Keras)\n",
    "    - Search over different dropout rates and L2 regularization strengths\n",
    "        - Training Set Classification Report (old vs new)\n",
    "        - Test Set Classification Report (old vs new)\n",
    "        - Conclusion\n",
    "- Further Fine-tuning\n",
    "    - Search over different dropout, L2 reg, activation, epochs, and batch_size\n",
    "        - Training Set Classification Report (old vs new)\n",
    "        - Test Set Classification Report (old vs new)\n",
    "        - Conclusion\n",
    "        - Save the model with best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87834a2e-27b4-4268-8890-7b7a1d571d6a",
   "metadata": {},
   "source": [
    "# Address Overfitting (Regularization, Early Stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b517953-78df-4114-8e1d-4ee939bbd75c",
   "metadata": {},
   "source": [
    "**Address Overfitting**:\n",
    "   - **Regularization**: Implement regularization techniques such as dropout, L1/L2 regularization to prevent overfitting.\n",
    "   - **Early Stopping**: Use early stopping during training to halt training once the model's performance on a validation set stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f66892c3-b3e2-4e59-94c5-6180606c3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.3085 - loss: 2.1615 - val_accuracy: 0.4346 - val_loss: 1.6651\n",
      "Epoch 2/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4070 - loss: 1.6365 - val_accuracy: 0.4592 - val_loss: 1.4336\n",
      "Epoch 3/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4341 - loss: 1.4374 - val_accuracy: 0.4649 - val_loss: 1.3290\n",
      "Epoch 4/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4472 - loss: 1.3492 - val_accuracy: 0.4869 - val_loss: 1.2744\n",
      "Epoch 5/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4503 - loss: 1.3000 - val_accuracy: 0.4788 - val_loss: 1.2473\n",
      "Epoch 6/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4611 - loss: 1.2693 - val_accuracy: 0.4820 - val_loss: 1.2285\n",
      "Epoch 7/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4739 - loss: 1.2519 - val_accuracy: 0.4902 - val_loss: 1.2146\n",
      "Epoch 8/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4781 - loss: 1.2442 - val_accuracy: 0.4967 - val_loss: 1.2108\n",
      "Epoch 9/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4713 - loss: 1.2292 - val_accuracy: 0.4967 - val_loss: 1.2029\n",
      "Epoch 10/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4768 - loss: 1.2319 - val_accuracy: 0.4959 - val_loss: 1.2029\n",
      "Epoch 11/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4851 - loss: 1.2271 - val_accuracy: 0.5025 - val_loss: 1.1963\n",
      "Epoch 12/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4849 - loss: 1.2176 - val_accuracy: 0.5016 - val_loss: 1.1928\n",
      "Epoch 13/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4886 - loss: 1.2197 - val_accuracy: 0.5049 - val_loss: 1.1892\n",
      "Epoch 14/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4882 - loss: 1.2161 - val_accuracy: 0.5074 - val_loss: 1.1935\n",
      "Epoch 15/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4776 - loss: 1.2101 - val_accuracy: 0.5098 - val_loss: 1.1841\n",
      "Epoch 16/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4818 - loss: 1.2084 - val_accuracy: 0.5065 - val_loss: 1.1880\n",
      "Epoch 17/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4964 - loss: 1.2070 - val_accuracy: 0.5065 - val_loss: 1.1844\n",
      "Epoch 18/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4748 - loss: 1.2085 - val_accuracy: 0.5131 - val_loss: 1.1859\n",
      "Epoch 19/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4901 - loss: 1.2045 - val_accuracy: 0.5082 - val_loss: 1.1862\n",
      "Epoch 20/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4830 - loss: 1.2112 - val_accuracy: 0.5049 - val_loss: 1.1807\n",
      "Epoch 21/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4857 - loss: 1.2091 - val_accuracy: 0.5131 - val_loss: 1.1796\n",
      "Epoch 22/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4785 - loss: 1.1984 - val_accuracy: 0.5155 - val_loss: 1.1807\n",
      "Epoch 23/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4882 - loss: 1.2016 - val_accuracy: 0.5041 - val_loss: 1.1806\n",
      "Epoch 24/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4826 - loss: 1.2067 - val_accuracy: 0.5172 - val_loss: 1.1719\n",
      "Epoch 25/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4907 - loss: 1.1971 - val_accuracy: 0.5180 - val_loss: 1.1713\n",
      "Epoch 26/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4908 - loss: 1.1978 - val_accuracy: 0.5114 - val_loss: 1.1772\n",
      "Epoch 27/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4882 - loss: 1.1973 - val_accuracy: 0.5188 - val_loss: 1.1733\n",
      "Epoch 28/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4801 - loss: 1.2034 - val_accuracy: 0.5147 - val_loss: 1.1756\n",
      "Epoch 29/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4943 - loss: 1.1968 - val_accuracy: 0.5139 - val_loss: 1.1758\n",
      "Epoch 30/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4931 - loss: 1.1932 - val_accuracy: 0.5253 - val_loss: 1.1691\n",
      "Epoch 31/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4963 - loss: 1.1974 - val_accuracy: 0.5163 - val_loss: 1.1676\n",
      "Epoch 32/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4940 - loss: 1.1954 - val_accuracy: 0.5237 - val_loss: 1.1699\n",
      "Epoch 33/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4981 - loss: 1.1910 - val_accuracy: 0.5163 - val_loss: 1.1694\n",
      "Epoch 34/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4823 - loss: 1.1954 - val_accuracy: 0.5204 - val_loss: 1.1700\n",
      "Epoch 35/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4970 - loss: 1.1985 - val_accuracy: 0.5139 - val_loss: 1.1704\n",
      "Epoch 36/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4851 - loss: 1.1894 - val_accuracy: 0.5106 - val_loss: 1.1717\n",
      "Epoch 37/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4950 - loss: 1.1844 - val_accuracy: 0.5229 - val_loss: 1.1638\n",
      "Epoch 38/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4899 - loss: 1.1907 - val_accuracy: 0.5188 - val_loss: 1.1675\n",
      "Epoch 39/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4929 - loss: 1.1856 - val_accuracy: 0.5196 - val_loss: 1.1667\n",
      "Epoch 40/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4956 - loss: 1.1913 - val_accuracy: 0.5065 - val_loss: 1.1717\n",
      "Epoch 41/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5049 - loss: 1.1929 - val_accuracy: 0.5082 - val_loss: 1.1726\n",
      "Epoch 42/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4975 - loss: 1.1821 - val_accuracy: 0.5188 - val_loss: 1.1670\n",
      "Epoch 43/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4913 - loss: 1.1832 - val_accuracy: 0.5049 - val_loss: 1.1725\n",
      "Epoch 44/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4996 - loss: 1.1862 - val_accuracy: 0.5074 - val_loss: 1.1671\n",
      "Epoch 45/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4978 - loss: 1.1864 - val_accuracy: 0.5090 - val_loss: 1.1648\n",
      "Epoch 46/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5006 - loss: 1.1885 - val_accuracy: 0.5090 - val_loss: 1.1680\n",
      "Epoch 47/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5020 - loss: 1.1851 - val_accuracy: 0.5155 - val_loss: 1.1659\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "import tensorflow as tf\n",
    "import random\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train_split_dimentionality_reducted.csv')\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df.drop(['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D'], axis=1)\n",
    "y = df[['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the ANN model with regularization and dropout\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')  # 4 output nodes for the 4 target classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d96b1b4f-6586-4e44-8fdd-1c07a967a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4951 - loss: 1.1679\n",
      "Train loss: 1.1601309776306152\n",
      "Train accuracy: 0.507557213306427\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5283 - loss: 1.1662\n",
      "Test loss: 1.1638070344924927\n",
      "Test accuracy: 0.5228758454322815\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Training Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.43      0.43      0.43      1230\n",
      "Segmentation_B       0.41      0.23      0.30      1160\n",
      "Segmentation_C       0.54      0.54      0.54      1159\n",
      "Segmentation_D       0.58      0.78      0.66      1347\n",
      "\n",
      "      accuracy                           0.51      4896\n",
      "     macro avg       0.49      0.50      0.48      4896\n",
      "  weighted avg       0.49      0.51      0.49      4896\n",
      "\n",
      "Test Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.46      0.45      0.45       309\n",
      "Segmentation_B       0.44      0.25      0.32       283\n",
      "Segmentation_C       0.57      0.57      0.57       292\n",
      "Segmentation_D       0.57      0.78      0.66       340\n",
      "\n",
      "      accuracy                           0.52      1224\n",
      "     macro avg       0.51      0.51      0.50      1224\n",
      "  weighted avg       0.51      0.52      0.51      1224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print('Train loss:', train_loss)\n",
    "print('Train accuracy:', train_accuracy)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Predict the classes for training data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n",
    "y_train_labels = np.argmax(np.array(y_train), axis=1)\n",
    "\n",
    "# Predict the classes for test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "y_test_labels = np.argmax(np.array(y_test), axis=1)\n",
    "\n",
    "# Print the classification report for the training set\n",
    "target_names = ['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']\n",
    "print(\"Training Set Classification Report\")\n",
    "print(classification_report(y_train_labels, y_train_pred_labels, target_names=target_names))\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Test Set Classification Report\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0dc78-0ab9-4330-b36d-925d33ebfd8f",
   "metadata": {},
   "source": [
    "## Conclusion from the Results\n",
    "\n",
    "### Overview of Metrics\n",
    "\n",
    "| Metric               | Previous Result | New Result                              |\n",
    "|----------------------|-----------------|-----------------------------------------|\n",
    "| **Training Loss**    | 0.8918          | 1.1601                                  |\n",
    "| **Training Accuracy**| 0.6168          | 0.5076                                  |\n",
    "| **Test Loss**        | 1.1721          | 1.1638                                  |\n",
    "| **Test Accuracy**    | 0.5008          | 0.5229                                  |\n",
    "\n",
    "### Training Set Classification Report (old vs new)\n",
    "\n",
    "| Segmentation      | Precision (Previous) | Recall (Previous) | F1-Score (Previous) | Support (Previous) | Precision (New) | Recall (New) | F1-Score (New) | Support (New) |\n",
    "|-------------------|----------------------|-------------------|---------------------|--------------------|-----------------|--------------|----------------|---------------|\n",
    "| Segmentation_A    | 0.58                 | 0.54              | 0.56                | 1230               | 0.43            | 0.43         | 0.43           | 1230          |\n",
    "| Segmentation_B    | 0.55                 | 0.47              | 0.51                | 1160               | 0.41            | 0.23         | 0.30           | 1160          |\n",
    "| Segmentation_C    | 0.65                 | 0.59              | 0.62                | 1159               | 0.54            | 0.54         | 0.54           | 1159          |\n",
    "| Segmentation_D    | 0.66                 | 0.84              | 0.74                | 1347               | 0.58            | 0.78         | 0.66           | 1347          |\n",
    "| **Overall Accuracy** |                   |                   | 0.62                | 4896               |                 |              | 0.51           | 4896          |\n",
    "| **Macro Avg**     | 0.61                 | 0.61              | 0.61                | 4896               | 0.49            | 0.50         | 0.48           | 4896          |\n",
    "| **Weighted Avg**  | 0.61                 | 0.62              | 0.61                | 4896               | 0.49            | 0.51         | 0.49           | 4896          |\n",
    "\n",
    "### Test Set Classification Report (old vs new)\n",
    "\n",
    "| Segmentation      | Precision (Previous) | Recall (Previous) | F1-Score (Previous) | Support (Previous) | Precision (New) | Recall (New) | F1-Score (New) | Support (New) |\n",
    "|-------------------|----------------------|-------------------|---------------------|--------------------|-----------------|--------------|----------------|---------------|\n",
    "| Segmentation_A    | 0.42                 | 0.38              | 0.40                | 309                | 0.46            | 0.45         | 0.45           | 309           |\n",
    "| Segmentation_B    | 0.39                 | 0.34              | 0.36                | 283                | 0.44            | 0.25         | 0.32           | 283           |\n",
    "| Segmentation_C    | 0.59                 | 0.53              | 0.56                | 292                | 0.57            | 0.57         | 0.57           | 292           |\n",
    "| Segmentation_D    | 0.56                 | 0.72              | 0.63                | 340                | 0.57            | 0.78         | 0.66           | 340           |\n",
    "| **Overall Accuracy** |                   |                   | 0.50                | 1224               |                 |              | 0.52           | 1224          |\n",
    "| **Macro Avg**     | 0.49                 | 0.49              | 0.49                | 1224               | 0.51            | 0.51         | 0.50           | 1224          |\n",
    "| **Weighted Avg**  | 0.49                 | 0.50              | 0.49                | 1224               | 0.51            | 0.52         | 0.51           | 1224          |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Overfitting Mitigation**: \n",
    "   - The regularization techniques and early stopping appear to have reduced overfitting, as indicated by the more consistent performance between the training and test sets. However, the overall accuracy has decreased, suggesting that the model may now be underfitting.\n",
    "\n",
    "2. **Class Performance**:\n",
    "   - **Segmentation_D**: Maintains relatively high precision, recall, and F1-score, indicating the model's robustness in predicting this class.\n",
    "   - **Segmentation_B**: Shows a drop in recall and F1-score, suggesting that the model still struggles to correctly identify instances of this class.\n",
    "\n",
    "3. **Model Performance**:\n",
    "   - **Training Performance**: The new model shows lower performance on the training set compared to the previous one, indicating the impact of regularization.\n",
    "   - **Test Performance**: The new model's performance on the test set is slightly lower, which might be an indicator of underfitting due to the regularization being too strong or the need for more epochs.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Further Tuning Regularization**:\n",
    "   - Adjust the dropout rate and L2 regularization strength to find a balance between overfitting and underfitting. Experiment with dropout rates between 0.2 and 0.5 and L2 regularization values between 0.001 and 0.01.\n",
    "\n",
    "2. **Extended Training**:\n",
    "   - Increase the number of epochs with early stopping to ensure the model has sufficient time to converge.\n",
    "\n",
    "3. **Hyperparameter Tuning**:\n",
    "   - Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV to find the optimal set of hyperparameters for the model.\n",
    "\n",
    "4. **Model Complexity**:\n",
    "   - Experiment with different types of layers, to improve the model's capacity to learn from the data.\n",
    "\n",
    "5. **Cross-Validation**:\n",
    "   - Implement cross-validation to ensure that the model's performance is consistent across different subsets of the data.\n",
    "\n",
    "By addressing these areas, we can further improve the model's performance and its ability to generalize to unseen data.\n",
    "odel's performance and its ability to generalize to unseen data.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fae504-4881-4841-a4ff-7575126fdabd",
   "metadata": {},
   "source": [
    "# Address Underfitting (RandomizedSearchCV with Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556ff75-ff7e-4abc-bce4-6a72671a7b97",
   "metadata": {},
   "source": [
    "## Search over different dropout rates and L2 regularization strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51acc56-d474-4401-acd1-a215697bfc9e",
   "metadata": {},
   "source": [
    "what changed:\n",
    "- Search over different dropout rates and L2 regularization strengths added\n",
    "- Early stopping patience changed from 10 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fae6d73f-8241-4d3c-bdfb-6de422b64c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best: 0.510825 using {'model__l2_reg': 0.005, 'model__dropout_rate': 0.4}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "import tensorflow as tf\n",
    "import random\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train_split_dimentionality_reducted.csv')\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df.drop(['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D'], axis=1)\n",
    "y = df[['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y to categorical\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define the function to create the model\n",
    "def create_model(dropout_rate=0.2, l2_reg=0.01):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using the KerasClassifier\n",
    "model = KerasClassifier(model=create_model, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_dist = {\n",
    "    'model__dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'model__l2_reg': [0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3, \n",
    "                                   verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_result = random_search.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best: %f using %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n",
    "\n",
    "# Train the best model\n",
    "best_params = random_search_result.best_params_\n",
    "best_model = create_model(dropout_rate=best_params['model__dropout_rate'], l2_reg=best_params['model__l2_reg'])\n",
    "history = best_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a960a5-5815-430d-a4f5-922da13d33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.126171588897705\n",
      "Train accuracy: 0.5255310535430908\n",
      "Test loss: 1.1389477252960205\n",
      "Test accuracy: 0.523692786693573\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Training Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.45      0.46      0.46      1230\n",
      "Segmentation_B       0.44      0.26      0.33      1160\n",
      "Segmentation_C       0.57      0.53      0.55      1159\n",
      "Segmentation_D       0.58      0.81      0.68      1347\n",
      "\n",
      "      accuracy                           0.53      4896\n",
      "     macro avg       0.51      0.52      0.50      4896\n",
      "  weighted avg       0.51      0.53      0.51      4896\n",
      "\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Test Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.43      0.44      0.43       309\n",
      "Segmentation_B       0.44      0.26      0.33       283\n",
      "Segmentation_C       0.60      0.55      0.57       292\n",
      "Segmentation_D       0.58      0.80      0.67       340\n",
      "\n",
      "      accuracy                           0.52      1224\n",
      "     macro avg       0.51      0.51      0.50      1224\n",
      "  weighted avg       0.51      0.52      0.51      1224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss:', train_loss)\n",
    "print('Train accuracy:', train_accuracy)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for the training set\n",
    "y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Print the classification report for the training set\n",
    "print(\"Training Set Classification Report\")\n",
    "print(classification_report(y_train_labels, y_train_pred_labels, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for the test set\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Test Set Classification Report\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4e13a-367f-43af-82b9-8da2bb81ec6a",
   "metadata": {},
   "source": [
    "## Training Set Classification Report (old vs new)\r\n",
    "\r\n",
    "| Segmentation  | Precision (Previous) | Recall (Previous) | F1-Score (Previous) | Support (Previous) | Precision (New) | Recall (New) | F1-Score (New) | Support (New) |\r\n",
    "|---------------|----------------------|-------------------|---------------------|--------------------|-----------------|--------------|----------------|---------------|\r\n",
    "| Segmentation_A| 0.43                 | 0.43              | 0.43                | 1230               | 0.45            | 0.46         | 0.46           | 1230          |\r\n",
    "| Segmentation_B| 0.41                 | 0.23              | 0.30                | 1160               | 0.44            | 0.26         | 0.33           | 1160          |\r\n",
    "| Segmentation_C| 0.54                 | 0.54              | 0.54                | 1159               | 0.57            | 0.53         | 0.55           | 1159          |\r\n",
    "| Segmentation_D| 0.58                 | 0.78              | 0.66                | 1347               | 0.58            | 0.81         | 0.68           | 1347          |\r\n",
    "| Accuracy      |                      |                   | 0.51                | 4896               |                 |              | 0.53           | 4896          |\r\n",
    "| Macro Avg     | 0.49                 | 0.50              | 0.48                | 4896               | 0.51            | 0.52         | 0.50           | 4896          |\r\n",
    "| Weighted Avg  | 0.49                 | 0.51              | 0.49                | 4896               | 0.51            | 0.53         | 0.51           | 4896          |\r\n",
    "\r\n",
    "## Test Set Classification Report (old vs new)\r\n",
    "\r\n",
    "| Segmentation  | Precision (Previous) | Recall (Previous) | F1-Score (Previous) | Support (Previous) | Precision (New) | Recall (New) | F1-Score (New) | Support (New) |\r\n",
    "|---------------|----------------------|-------------------|---------------------|--------------------|-----------------|--------------|----------------|---------------|\r\n",
    "| Segmentation_A| 0.46                 | 0.45              | 0.45                | 309                | 0.43            | 0.44         | 0.43           | 309           |\r\n",
    "| Segmentation_B| 0.44                 | 0.25              | 0.32                | 283                | 0.44            | 0.26         | 0.33           | 283           |\r\n",
    "| Segmentation_C| 0.57                 | 0.57              | 0.57                | 292                | 0.60            | 0.55         | 0.57           | 292           |\r\n",
    "| Segmentation_D| 0.57                 | 0.78              | 0.66                | 340                | 0.58            | 0.80         | 0.67           | 340           |\r\n",
    "| Accuracy      |                      |                   | 0.52                | 1224               |                 |              | 0.52           | 1224          |\r\n",
    "| Macro Avg     | 0.51                 | 0.51              | 0.50                | 1224               | 0.51            | 0.51         | 0.50           | 1224          |\r\n",
    "| Weighted Avg  | 0.51                 | 0.52              | 0.51                | 1224               | 0.51            | 0.52         | 0.51           | 1224          |\r\n",
    "\r\n",
    "### Overview of Metrics\r\n",
    "\r\n",
    "| Metric               | Previous Result | New Result                              |\r\n",
    "|----------------------|-----------------|-----------------------------------------|\r\n",
    "| **Training Loss**    | 1.1601          | 1.1262                                  |\r\n",
    "| **Training Accuracy**| 0.5076          | 0.5255                                  |\r\n",
    "| **Test Loss**        | 1.1638          | 1.1389                                  |\r\n",
    "| **Test Accuracy**    | 0.5229          | 0.5237                                  |\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "- **Training Set**:\r\n",
    "  - The new model shows a slight improvement in overall accuracy from 0.51 to 0.53.\r\n",
    "  - Precision, recall, and F1-scores have slightly increased for Segmentation_A and Segmentation_B but decreased slightly for Segmentation_C and Segmentation_D.\r\n",
    "  - The macro and weighted averages for precision, recall, and F1-score have also improved slightly.\r\n",
    "\r\n",
    "- **Test Set**:\r\n",
    "  - The overall accuracy on the test set increased slightly from 0.52 to 0.53.\r\n",
    "  - Precision, recall, and F1-scores have increased for Segmentation_C and Segmentation_D but slightly decreased for Segmentation_A and Segmentation_B.\r\n",
    "  - Macro and weighted averages for precision, recall, and F1-score show slight improvements.\r\n",
    "\r\n",
    "The slight improvements in both training and test set metrics indicate that the chosen best hyperparameters (dropout rate and L2 regularization) have positively impacted its performance, making it more generalizable and defying the segments correctly.\r\n",
    "t more generalizable and better at classifying the segments correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16fb35-fec9-4717-ae2b-48c1f5399ca7",
   "metadata": {},
   "source": [
    "## Further Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7f9f2-87f8-4671-b5ce-76cbd725af2a",
   "metadata": {},
   "source": [
    "### Search over different dropout, L2 reg, activation, epochs, and batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a8713-3c8d-450c-945b-59c42e3f78c4",
   "metadata": {},
   "source": [
    "Updated code that includes activation, epochs, and batch_size in the random search, and uses all available cores.\n",
    "Also, patience in reduce_lr was increased from 5 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de6dc018-4e8a-4ce3-8a85-5461c0089491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best: 0.507966 using {'model__learning_rate': 0.001, 'model__l2_reg': 0.01, 'model__dropout_rate': 0.2, 'model__activation': 'relu', 'epochs': 100, 'batch_size': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 14:37:39,758 - INFO - Epoch 1 completed. Loss: 1.8887606859207153, Accuracy: 0.40992647409439087, Val Loss: 1.583421230316162, Val Accuracy: 0.4591503143310547\n",
      "2024-07-18 14:37:41,678 - INFO - Epoch 2 completed. Loss: 1.4756624698638916, Accuracy: 0.45812907814979553, Val Loss: 1.3693695068359375, Val Accuracy: 0.4820261299610138\n",
      "2024-07-18 14:37:42,963 - INFO - Epoch 3 completed. Loss: 1.337983250617981, Accuracy: 0.464665025472641, Val Loss: 1.2838596105575562, Val Accuracy: 0.47875815629959106\n",
      "2024-07-18 14:37:44,003 - INFO - Epoch 4 completed. Loss: 1.2702945470809937, Accuracy: 0.47732841968536377, Val Loss: 1.2408149242401123, Val Accuracy: 0.49754902720451355\n",
      "2024-07-18 14:37:45,370 - INFO - Epoch 5 completed. Loss: 1.246335506439209, Accuracy: 0.4818218946456909, Val Loss: 1.21883225440979, Val Accuracy: 0.49754902720451355\n",
      "2024-07-18 14:37:46,678 - INFO - Epoch 6 completed. Loss: 1.225836157798767, Accuracy: 0.48631536960601807, Val Loss: 1.2066571712493896, Val Accuracy: 0.49754902720451355\n",
      "2024-07-18 14:37:48,057 - INFO - Epoch 7 completed. Loss: 1.2124428749084473, Accuracy: 0.4908088147640228, Val Loss: 1.1936655044555664, Val Accuracy: 0.49754902720451355\n",
      "2024-07-18 14:37:49,366 - INFO - Epoch 8 completed. Loss: 1.204646348953247, Accuracy: 0.4895833432674408, Val Loss: 1.1885775327682495, Val Accuracy: 0.5016340017318726\n",
      "2024-07-18 14:37:50,691 - INFO - Epoch 9 completed. Loss: 1.1982086896896362, Accuracy: 0.49489378929138184, Val Loss: 1.1843020915985107, Val Accuracy: 0.5106208920478821\n",
      "2024-07-18 14:37:52,010 - INFO - Epoch 10 completed. Loss: 1.1942280530929565, Accuracy: 0.49489378929138184, Val Loss: 1.182569146156311, Val Accuracy: 0.508169949054718\n",
      "2024-07-18 14:37:53,350 - INFO - Epoch 11 completed. Loss: 1.1870023012161255, Accuracy: 0.4916258156299591, Val Loss: 1.1771639585494995, Val Accuracy: 0.5098039507865906\n",
      "2024-07-18 14:37:54,596 - INFO - Epoch 12 completed. Loss: 1.182011604309082, Accuracy: 0.5028594732284546, Val Loss: 1.172864317893982, Val Accuracy: 0.5114378929138184\n",
      "2024-07-18 14:37:55,961 - INFO - Epoch 13 completed. Loss: 1.1879377365112305, Accuracy: 0.49693626165390015, Val Loss: 1.1720536947250366, Val Accuracy: 0.5114378929138184\n",
      "2024-07-18 14:37:57,271 - INFO - Epoch 14 completed. Loss: 1.1832259893417358, Accuracy: 0.49959149956703186, Val Loss: 1.1683967113494873, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:37:58,652 - INFO - Epoch 15 completed. Loss: 1.182181715965271, Accuracy: 0.500612735748291, Val Loss: 1.1672711372375488, Val Accuracy: 0.5187908411026001\n",
      "2024-07-18 14:37:59,700 - INFO - Epoch 16 completed. Loss: 1.1764663457870483, Accuracy: 0.5026552081108093, Val Loss: 1.1627916097640991, Val Accuracy: 0.5187908411026001\n",
      "2024-07-18 14:38:00,653 - INFO - Epoch 17 completed. Loss: 1.1774868965148926, Accuracy: 0.4973447620868683, Val Loss: 1.1641395092010498, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:38:02,126 - INFO - Epoch 18 completed. Loss: 1.1726479530334473, Accuracy: 0.5014297366142273, Val Loss: 1.1611171960830688, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:38:03,250 - INFO - Epoch 19 completed. Loss: 1.1728579998016357, Accuracy: 0.4973447620868683, Val Loss: 1.1582742929458618, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:38:04,559 - INFO - Epoch 20 completed. Loss: 1.1718347072601318, Accuracy: 0.5024510025978088, Val Loss: 1.1583023071289062, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:38:05,657 - INFO - Epoch 21 completed. Loss: 1.1718367338180542, Accuracy: 0.5057189464569092, Val Loss: 1.1607226133346558, Val Accuracy: 0.5269607901573181\n",
      "2024-07-18 14:38:06,982 - INFO - Epoch 22 completed. Loss: 1.1723546981811523, Accuracy: 0.5010212659835815, Val Loss: 1.1585346460342407, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:38:08,268 - INFO - Epoch 23 completed. Loss: 1.172730565071106, Accuracy: 0.5032680034637451, Val Loss: 1.1568390130996704, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:38:09,519 - INFO - Epoch 24 completed. Loss: 1.1664648056030273, Accuracy: 0.5089869499206543, Val Loss: 1.1541789770126343, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:38:10,590 - INFO - Epoch 25 completed. Loss: 1.1673213243484497, Accuracy: 0.5042892098426819, Val Loss: 1.1547414064407349, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:38:11,999 - INFO - Epoch 26 completed. Loss: 1.163117527961731, Accuracy: 0.5085784196853638, Val Loss: 1.1527503728866577, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:38:13,274 - INFO - Epoch 27 completed. Loss: 1.1689914464950562, Accuracy: 0.5063316822052002, Val Loss: 1.1502810716629028, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:38:14,419 - INFO - Epoch 28 completed. Loss: 1.1599125862121582, Accuracy: 0.5114378929138184, Val Loss: 1.150954246520996, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:38:15,877 - INFO - Epoch 29 completed. Loss: 1.1663854122161865, Accuracy: 0.5049019455909729, Val Loss: 1.1523886919021606, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:38:17,086 - INFO - Epoch 30 completed. Loss: 1.16318678855896, Accuracy: 0.5046977400779724, Val Loss: 1.1505522727966309, Val Accuracy: 0.5302287340164185\n",
      "2024-07-18 14:38:18,415 - INFO - Epoch 31 completed. Loss: 1.1638013124465942, Accuracy: 0.5034722089767456, Val Loss: 1.1490051746368408, Val Accuracy: 0.5277777910232544\n",
      "2024-07-18 14:38:19,748 - INFO - Epoch 32 completed. Loss: 1.1623650789260864, Accuracy: 0.5051062107086182, Val Loss: 1.150545358657837, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:38:21,096 - INFO - Epoch 33 completed. Loss: 1.1632438898086548, Accuracy: 0.5014297366142273, Val Loss: 1.1483304500579834, Val Accuracy: 0.5269607901573181\n",
      "2024-07-18 14:38:22,448 - INFO - Epoch 34 completed. Loss: 1.1597775220870972, Accuracy: 0.5069444179534912, Val Loss: 1.1446841955184937, Val Accuracy: 0.529411792755127\n",
      "2024-07-18 14:38:23,714 - INFO - Epoch 35 completed. Loss: 1.1605496406555176, Accuracy: 0.5044934749603271, Val Loss: 1.1464684009552002, Val Accuracy: 0.5277777910232544\n",
      "2024-07-18 14:38:25,214 - INFO - Epoch 36 completed. Loss: 1.160951018333435, Accuracy: 0.5079656839370728, Val Loss: 1.1464018821716309, Val Accuracy: 0.5310457348823547\n",
      "2024-07-18 14:38:26,394 - INFO - Epoch 37 completed. Loss: 1.156515121459961, Accuracy: 0.5140931606292725, Val Loss: 1.148490309715271, Val Accuracy: 0.5269607901573181\n",
      "2024-07-18 14:38:27,713 - INFO - Epoch 38 completed. Loss: 1.157210350036621, Accuracy: 0.5059232115745544, Val Loss: 1.1459980010986328, Val Accuracy: 0.5269607901573181\n",
      "2024-07-18 14:38:29,046 - INFO - Epoch 39 completed. Loss: 1.1568185091018677, Accuracy: 0.5091911554336548, Val Loss: 1.146447777748108, Val Accuracy: 0.5326797366142273\n",
      "2024-07-18 14:38:30,368 - INFO - Epoch 40 completed. Loss: 1.1566191911697388, Accuracy: 0.5089869499206543, Val Loss: 1.1442627906799316, Val Accuracy: 0.529411792755127\n",
      "2024-07-18 14:38:31,590 - INFO - Epoch 41 completed. Loss: 1.1541786193847656, Accuracy: 0.5083741545677185, Val Loss: 1.145624041557312, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:38:33,003 - INFO - Epoch 42 completed. Loss: 1.1572054624557495, Accuracy: 0.5083741545677185, Val Loss: 1.143158197402954, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:38:34,309 - INFO - Epoch 43 completed. Loss: 1.1545732021331787, Accuracy: 0.5110294222831726, Val Loss: 1.1420867443084717, Val Accuracy: 0.5302287340164185\n",
      "2024-07-18 14:38:35,408 - INFO - Epoch 44 completed. Loss: 1.1533291339874268, Accuracy: 0.5130718946456909, Val Loss: 1.141046404838562, Val Accuracy: 0.5261437892913818\n",
      "2024-07-18 14:38:36,733 - INFO - Epoch 45 completed. Loss: 1.1520893573760986, Accuracy: 0.5051062107086182, Val Loss: 1.144274353981018, Val Accuracy: 0.5326797366142273\n",
      "2024-07-18 14:38:38,106 - INFO - Epoch 46 completed. Loss: 1.1521389484405518, Accuracy: 0.5067402124404907, Val Loss: 1.1417063474655151, Val Accuracy: 0.5285947918891907\n",
      "2024-07-18 14:38:39,418 - INFO - Epoch 47 completed. Loss: 1.1544899940490723, Accuracy: 0.5069444179534912, Val Loss: 1.1397892236709595, Val Accuracy: 0.529411792755127\n",
      "2024-07-18 14:38:40,806 - INFO - Epoch 48 completed. Loss: 1.149123191833496, Accuracy: 0.5171568393707275, Val Loss: 1.138781189918518, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:38:42,067 - INFO - Epoch 49 completed. Loss: 1.1562942266464233, Accuracy: 0.5091911554336548, Val Loss: 1.143243432044983, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:38:43,401 - INFO - Epoch 50 completed. Loss: 1.153916597366333, Accuracy: 0.5116421580314636, Val Loss: 1.1411774158477783, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:38:44,703 - INFO - Epoch 51 completed. Loss: 1.1456012725830078, Accuracy: 0.5130718946456909, Val Loss: 1.1401764154434204, Val Accuracy: 0.5277777910232544\n",
      "2024-07-18 14:38:46,056 - INFO - Epoch 52 completed. Loss: 1.1542819738388062, Accuracy: 0.5032680034637451, Val Loss: 1.1403316259384155, Val Accuracy: 0.5261437892913818\n",
      "2024-07-18 14:38:47,309 - INFO - Epoch 53 completed. Loss: 1.1501435041427612, Accuracy: 0.5065359473228455, Val Loss: 1.1382620334625244, Val Accuracy: 0.5269607901573181\n",
      "2024-07-18 14:38:48,663 - INFO - Epoch 54 completed. Loss: 1.149715542793274, Accuracy: 0.5095996856689453, Val Loss: 1.1419458389282227, Val Accuracy: 0.5261437892913818\n",
      "2024-07-18 14:38:50,041 - INFO - Epoch 55 completed. Loss: 1.1455612182617188, Accuracy: 0.5161356329917908, Val Loss: 1.1428625583648682, Val Accuracy: 0.5196078419685364\n",
      "2024-07-18 14:38:51,352 - INFO - Epoch 56 completed. Loss: 1.1482330560684204, Accuracy: 0.5089869499206543, Val Loss: 1.1407467126846313, Val Accuracy: 0.5326797366142273\n",
      "2024-07-18 14:38:52,660 - INFO - Epoch 57 completed. Loss: 1.1486810445785522, Accuracy: 0.5128676295280457, Val Loss: 1.138656735420227, Val Accuracy: 0.5261437892913818\n",
      "2024-07-18 14:38:53,786 - INFO - Epoch 58 completed. Loss: 1.146309494972229, Accuracy: 0.5118464231491089, Val Loss: 1.1403573751449585, Val Accuracy: 0.529411792755127\n",
      "2024-07-18 14:38:55,116 - INFO - Epoch 59 completed. Loss: 1.1466459035873413, Accuracy: 0.5042892098426819, Val Loss: 1.139918565750122, Val Accuracy: 0.5285947918891907\n",
      "2024-07-18 14:38:56,441 - INFO - Epoch 60 completed. Loss: 1.1499252319335938, Accuracy: 0.5053104758262634, Val Loss: 1.139672875404358, Val Accuracy: 0.531862735748291\n",
      "2024-07-18 14:38:57,782 - INFO - Epoch 61 completed. Loss: 1.1470160484313965, Accuracy: 0.5130718946456909, Val Loss: 1.1379940509796143, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:38:59,128 - INFO - Epoch 62 completed. Loss: 1.1442457437515259, Accuracy: 0.5120506286621094, Val Loss: 1.1409164667129517, Val Accuracy: 0.529411792755127\n",
      "2024-07-18 14:39:00,431 - INFO - Epoch 63 completed. Loss: 1.1459161043167114, Accuracy: 0.5069444179534912, Val Loss: 1.1439846754074097, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:39:01,799 - INFO - Epoch 64 completed. Loss: 1.146492600440979, Accuracy: 0.5147058963775635, Val Loss: 1.135684609413147, Val Accuracy: 0.5285947918891907\n",
      "2024-07-18 14:39:03,127 - INFO - Epoch 65 completed. Loss: 1.1454224586486816, Accuracy: 0.514910101890564, Val Loss: 1.1374374628067017, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:04,102 - INFO - Epoch 66 completed. Loss: 1.1460787057876587, Accuracy: 0.5126634240150452, Val Loss: 1.1393816471099854, Val Accuracy: 0.5277777910232544\n",
      "2024-07-18 14:39:05,544 - INFO - Epoch 67 completed. Loss: 1.1466201543807983, Accuracy: 0.5100081562995911, Val Loss: 1.138138771057129, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:06,898 - INFO - Epoch 68 completed. Loss: 1.143746018409729, Accuracy: 0.5179738402366638, Val Loss: 1.1386632919311523, Val Accuracy: 0.5310457348823547\n",
      "2024-07-18 14:39:07,804 - INFO - Epoch 69 completed. Loss: 1.1449958086013794, Accuracy: 0.5132761597633362, Val Loss: 1.1377639770507812, Val Accuracy: 0.5269607901573181\n",
      "2024-07-18 14:39:09,098 - INFO - Epoch 70 completed. Loss: 1.1436799764633179, Accuracy: 0.5147058963775635, Val Loss: 1.1368718147277832, Val Accuracy: 0.5310457348823547\n",
      "2024-07-18 14:39:10,377 - INFO - Epoch 71 completed. Loss: 1.1460901498794556, Accuracy: 0.5118464231491089, Val Loss: 1.141907811164856, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:39:11,200 - INFO - Epoch 72 completed. Loss: 1.144622564315796, Accuracy: 0.5202205777168274, Val Loss: 1.1376898288726807, Val Accuracy: 0.5351307392120361\n",
      "2024-07-18 14:39:12,498 - INFO - Epoch 73 completed. Loss: 1.1424250602722168, Accuracy: 0.516952633857727, Val Loss: 1.1411579847335815, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:13,875 - INFO - Epoch 74 completed. Loss: 1.146401047706604, Accuracy: 0.5122548937797546, Val Loss: 1.1352548599243164, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:15,410 - INFO - Epoch 75 completed. Loss: 1.1413863897323608, Accuracy: 0.5153186321258545, Val Loss: 1.1378899812698364, Val Accuracy: 0.5253267884254456\n",
      "2024-07-18 14:39:16,810 - INFO - Epoch 76 completed. Loss: 1.1444239616394043, Accuracy: 0.5138888955116272, Val Loss: 1.1336326599121094, Val Accuracy: 0.529411792755127\n",
      "2024-07-18 14:39:17,868 - INFO - Epoch 77 completed. Loss: 1.1455327272415161, Accuracy: 0.5071486830711365, Val Loss: 1.1400703191757202, Val Accuracy: 0.5343137383460999\n",
      "2024-07-18 14:39:19,200 - INFO - Epoch 78 completed. Loss: 1.1444391012191772, Accuracy: 0.5114378929138184, Val Loss: 1.136039137840271, Val Accuracy: 0.5212418437004089\n",
      "2024-07-18 14:39:20,532 - INFO - Epoch 79 completed. Loss: 1.1390377283096313, Accuracy: 0.516339898109436, Val Loss: 1.1360152959823608, Val Accuracy: 0.5204248428344727\n",
      "2024-07-18 14:39:21,654 - INFO - Epoch 80 completed. Loss: 1.142652153968811, Accuracy: 0.5153186321258545, Val Loss: 1.1428800821304321, Val Accuracy: 0.5204248428344727\n",
      "2024-07-18 14:39:22,952 - INFO - Epoch 81 completed. Loss: 1.1400129795074463, Accuracy: 0.5134803652763367, Val Loss: 1.134274959564209, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:24,110 - INFO - Epoch 82 completed. Loss: 1.1384717226028442, Accuracy: 0.5106208920478821, Val Loss: 1.1337192058563232, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:25,096 - INFO - Epoch 83 completed. Loss: 1.1391266584396362, Accuracy: 0.5196078419685364, Val Loss: 1.1361896991729736, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:26,005 - INFO - Epoch 84 completed. Loss: 1.1405043601989746, Accuracy: 0.5140931606292725, Val Loss: 1.139574646949768, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:39:27,454 - INFO - Epoch 85 completed. Loss: 1.1427741050720215, Accuracy: 0.508782684803009, Val Loss: 1.1391595602035522, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:39:28,683 - INFO - Epoch 86 completed. Loss: 1.139799952507019, Accuracy: 0.5175653696060181, Val Loss: 1.1351717710494995, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:29,546 - INFO - Epoch 87 completed. Loss: 1.1257336139678955, Accuracy: 0.516339898109436, Val Loss: 1.1277018785476685, Val Accuracy: 0.5261437892913818\n",
      "2024-07-18 14:39:30,515 - INFO - Epoch 88 completed. Loss: 1.1301569938659668, Accuracy: 0.5175653696060181, Val Loss: 1.1271286010742188, Val Accuracy: 0.5245097875595093\n",
      "2024-07-18 14:39:31,741 - INFO - Epoch 89 completed. Loss: 1.1252399682998657, Accuracy: 0.5290032625198364, Val Loss: 1.1255401372909546, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:33,143 - INFO - Epoch 90 completed. Loss: 1.1262660026550293, Accuracy: 0.5257353186607361, Val Loss: 1.1250001192092896, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:34,540 - INFO - Epoch 91 completed. Loss: 1.1227052211761475, Accuracy: 0.5234885811805725, Val Loss: 1.1253223419189453, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:35,668 - INFO - Epoch 92 completed. Loss: 1.1258145570755005, Accuracy: 0.522467315196991, Val Loss: 1.1249386072158813, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:36,482 - INFO - Epoch 93 completed. Loss: 1.1230223178863525, Accuracy: 0.5275735259056091, Val Loss: 1.1248868703842163, Val Accuracy: 0.5204248428344727\n",
      "2024-07-18 14:39:37,830 - INFO - Epoch 94 completed. Loss: 1.1241300106048584, Accuracy: 0.5247140526771545, Val Loss: 1.1241352558135986, Val Accuracy: 0.5228758454322815\n",
      "2024-07-18 14:39:39,283 - INFO - Epoch 95 completed. Loss: 1.1217972040176392, Accuracy: 0.5243055820465088, Val Loss: 1.124395728111267, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:40,101 - INFO - Epoch 96 completed. Loss: 1.1238702535629272, Accuracy: 0.5220588445663452, Val Loss: 1.1234933137893677, Val Accuracy: 0.5212418437004089\n",
      "2024-07-18 14:39:40,927 - INFO - Epoch 97 completed. Loss: 1.1205112934112549, Accuracy: 0.5228758454322815, Val Loss: 1.122696042060852, Val Accuracy: 0.5220588445663452\n",
      "2024-07-18 14:39:41,835 - INFO - Epoch 98 completed. Loss: 1.1202572584152222, Accuracy: 0.5275735259056091, Val Loss: 1.1235989332199097, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:43,137 - INFO - Epoch 99 completed. Loss: 1.1226606369018555, Accuracy: 0.5206291079521179, Val Loss: 1.123265027999878, Val Accuracy: 0.523692786693573\n",
      "2024-07-18 14:39:43,989 - INFO - Epoch 100 completed. Loss: 1.122989296913147, Accuracy: 0.5281862616539001, Val Loss: 1.1227467060089111, Val Accuracy: 0.5204248428344727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Custom callback for logging\n",
    "class LoggingCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logging.info(f\"Epoch {epoch + 1} completed. Loss: {logs['loss']}, Accuracy: {logs['accuracy']}, Val Loss: {logs['val_loss']}, Val Accuracy: {logs['val_accuracy']}\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train_split_dimentionality_reducted.csv')\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df.drop(['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D'], axis=1)\n",
    "y = df[['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y to categorical\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define the function to create the model\n",
    "def create_model(dropout_rate=0.2, l2_reg=0.01, learning_rate=0.001, activation='relu'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(64, activation=activation, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation=activation, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using the KerasClassifier\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_dist = {\n",
    "    'model__dropout_rate': [0.2, 0.4],\n",
    "    'model__l2_reg': [0.001, 0.01],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'epochs': [10, 50, 100],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "# Define early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)\n",
    "logging_callback = LoggingCallback()\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, scoring='accuracy', cv=3, \n",
    "                                   verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_result = random_search.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr, logging_callback])\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best: %f using %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n",
    "\n",
    "# Train the best model\n",
    "best_params = random_search_result.best_params_\n",
    "best_model = create_model(dropout_rate=best_params['model__dropout_rate'], l2_reg=best_params['model__l2_reg'], learning_rate=best_params['model__learning_rate'], activation=best_params['model__activation'])\n",
    "history = best_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr, logging_callback], verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64c9f891-1884-4f62-8ddf-dd62cb55e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.104968547821045\n",
      "Train accuracy: 0.5371732115745544\n",
      "Test loss: 1.122696042060852\n",
      "Test accuracy: 0.5220588445663452\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.47      0.49      0.48      1230\n",
      "Segmentation_B       0.44      0.32      0.37      1160\n",
      "Segmentation_C       0.58      0.54      0.56      1159\n",
      "Segmentation_D       0.61      0.76      0.68      1347\n",
      "\n",
      "      accuracy                           0.54      4896\n",
      "     macro avg       0.52      0.53      0.52      4896\n",
      "  weighted avg       0.53      0.54      0.53      4896\n",
      "\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Test Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.43      0.47      0.45       309\n",
      "Segmentation_B       0.42      0.27      0.33       283\n",
      "Segmentation_C       0.58      0.56      0.57       292\n",
      "Segmentation_D       0.60      0.75      0.67       340\n",
      "\n",
      "      accuracy                           0.52      1224\n",
      "     macro avg       0.51      0.51      0.50      1224\n",
      "  weighted avg       0.51      0.52      0.51      1224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss:', train_loss)\n",
    "print('Train accuracy:', train_accuracy)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for the training set\n",
    "y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Print the classification report for the training set\n",
    "print(\"Training Set Classification Report\")\n",
    "print(classification_report(y_train_labels, y_train_pred_labels, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for the test set\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Test Set Classification Report\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22979e-e4de-4c26-a60f-5ece56e86590",
   "metadata": {},
   "source": [
    "## Training Set Classification Report (old vs new)\r\n",
    "\r\n",
    "| Segmentation  | Precision (Previous) | Recall (Previous) | F1-Score (Previous) | Support (Previous) | Precision (New) | Recall (New) | F1-Score (New) | Support (New) |\r\n",
    "|---------------|----------------------|-------------------|---------------------|--------------------|-----------------|--------------|----------------|---------------|\r\n",
    "| Segmentation_A| 0.45                 | 0.46              | 0.46                | 1230               | 0.47            | 0.49         | 0.48           | 1230          |\r\n",
    "| Segmentation_B| 0.44                 | 0.26              | 0.33                | 1160               | 0.44            | 0.32         | 0.37           | 1160          |\r\n",
    "| Segmentation_C| 0.57                 | 0.53              | 0.55                | 1159               | 0.58            | 0.54         | 0.56           | 1159          |\r\n",
    "| Segmentation_D| 0.58                 | 0.81              | 0.68                | 1347               | 0.61            | 0.76         | 0.68           | 1347          |\r\n",
    "| **Accuracy**  |                      |                   | 0.53                | 4896               |                 |              | 0.54           | 4896          |\r\n",
    "| **Macro Avg** | 0.51                 | 0.52              | 0.50                | 4896               | 0.52            | 0.53         | 0.52           | 4896          |\r\n",
    "| **Weighted Avg** | 0.51              | 0.53              | 0.51                | 4896               | 0.53            | 0.54         | 0.53           | 4896          |\r\n",
    "\r\n",
    "## Test Set Classification Report (old vs new)\r\n",
    "\r\n",
    "| Segmentation  | Precision (Previous) | Recall (Previous) | F1-Score (Previous) | Support (Previous) | Precision (New) | Recall (New) | F1-Score (New) | Support (New) |\r\n",
    "|---------------|----------------------|-------------------|---------------------|--------------------|-----------------|--------------|----------------|---------------|\r\n",
    "| Segmentation_A| 0.43                 | 0.44              | 0.43                | 309                | 0.43            | 0.47         | 0.45           | 309           |\r\n",
    "| Segmentation_B| 0.44                 | 0.26              | 0.33                | 283                | 0.42            | 0.27         | 0.33           | 283           |\r\n",
    "| Segmentation_C| 0.60                 | 0.55              | 0.57                | 292                | 0.58            | 0.56         | 0.57           | 292           |\r\n",
    "| Segmentation_D| 0.58                 | 0.80              | 0.67                | 340                | 0.60            | 0.75         | 0.67           | 340           |\r\n",
    "| **Accuracy**  |                      |                   | 0.52                | 1224               |                 |              | 0.52           | 1224          |\r\n",
    "| **Macro Avg** | 0.51                 | 0.51              | 0.50                | 1224               | 0.51            | 0.51         | 0.50           | 1224          |\r\n",
    "| **Weighted Avg** | 0.51              | 0.52              | 0.51                | 1224               | 0.51            | 0.52         | 0.51           | 1224          |\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "- **Training Set**:\r\n",
    "  - The overall accuracy on the training set has slightly increased from 0.53 to 0.54.\r\n",
    "  - Precision, recall, and F1-scores have slightly improved for most segments, with Segmentation_A and Segmentation_D showing the most notable improvements.\r\n",
    "  - The macro and weighted averages for precision, recall, and F1-score have shown slight improvements.\r\n",
    "\r\n",
    "- **Test Set**:\r\n",
    "  - The overall accuracy on the test set has remained stable at 0.52.\r\n",
    "  - Precision, recall, and F1-scores have remained relatively stable for Segmentation_B, Segmentation_C, and Segmentation_D, with slight improvements in Segmentation_A.\r\n",
    "  - Macro and weighted averages for precision, recall, and F1-score have remained stable.\r\n",
    "\r\n",
    "The new model, with adjusted hyperparameters and learning rate adjustments, has shown a slight improvement in training performance but has maintained with slight improvements in Segmentation_A s\n",
    "\n",
    "**Best parameters are:**\n",
    "- {'model__learning_rate': 0.001, 'model__l2_reg': 0.01, 'model__dropout_rate': 0.2, 'model__activation': 'relu', 'epochs': 100, 'batch_size': 32}\n",
    "- early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "- reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)ficant improvements in performance.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a2d6d-997b-4d51-b8fb-ddf981201a05",
   "metadata": {},
   "source": [
    "## Save the model with best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75ec67-96fe-46c6-9e01-dd6f826f3bf8",
   "metadata": {},
   "source": [
    "Here is modified code with the best parameters applied directly and the model saved for future use:\n",
    "{'model__learning_rate': 0.001, 'model__l2_reg': 0.01, 'model__dropout_rate': 0.2, 'model__activation': 'relu', 'epochs': 100, 'batch_size': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20b64acb-791e-4923-b65f-15d2f502c55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train_split_dimentionality_reducted.csv')\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df.drop(['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D'], axis=1)\n",
    "y = df[['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y to categorical\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define the function to create the model with the best parameters\n",
    "def create_model(dropout_rate=0.2, l2_reg=0.01, learning_rate=0.001, activation='relu'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(64, activation=activation, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation=activation, kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model with the best parameters\n",
    "model = create_model(dropout_rate=0.2, l2_reg=0.01, learning_rate=0.001, activation='relu')\n",
    "\n",
    "# Define early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "# Save the model\n",
    "model.save('pretrained_ann_classification_model.keras')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fe268bc-599b-4e24-b7a4-7758ccac8ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1057605743408203\n",
      "Train accuracy: 0.5334967374801636\n",
      "Test loss: 1.1231225728988647\n",
      "Test accuracy: 0.5261437892913818\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Training Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.46      0.47      0.47      1230\n",
      "Segmentation_B       0.43      0.32      0.36      1160\n",
      "Segmentation_C       0.57      0.56      0.56      1159\n",
      "Segmentation_D       0.62      0.76      0.68      1347\n",
      "\n",
      "      accuracy                           0.53      4896\n",
      "     macro avg       0.52      0.53      0.52      4896\n",
      "  weighted avg       0.52      0.53      0.52      4896\n",
      "\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Test Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.44      0.46      0.45       309\n",
      "Segmentation_B       0.42      0.29      0.34       283\n",
      "Segmentation_C       0.58      0.57      0.57       292\n",
      "Segmentation_D       0.61      0.75      0.67       340\n",
      "\n",
      "      accuracy                           0.53      1224\n",
      "     macro avg       0.51      0.52      0.51      1224\n",
      "  weighted avg       0.51      0.53      0.52      1224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss:', train_loss)\n",
    "print('Train accuracy:', train_accuracy)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for the training set\n",
    "y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Print the classification report for the training set\n",
    "print(\"Training Set Classification Report\")\n",
    "print(classification_report(y_train_labels, y_train_pred_labels, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for the test set\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Test Set Classification Report\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95b80c-b3b7-4811-86f4-956ff82f9ca6",
   "metadata": {},
   "source": [
    "**Why result is different when I use best parameters directly from the run where best parameters were found?**\n",
    "When we directly use the best parameters from a hyperparameter tuning run, you might expect the results to be similar or even better. However, there can be several reasons for discrepancies in the results:\n",
    "\n",
    "- Random Initialization: Neural networks, especially with Keras and TensorFlow, involve random initialization of weights. Even with the same hyperparameters, different initial weights can lead to different convergence behavior and thus different final results.\n",
    "\n",
    "- Stochastic Nature of Training: The training process involves random shuffling of data and can behave differently in different runs due to the stochastic nature of optimization algorithms like Adam. This can lead to different final results even with the same hyperparameters.\n",
    "\n",
    "- Regularization: Dropout is a stochastic regularization technique. The neurons are randomly dropped during training, leading to variations in the training process and final model performance.\n",
    "\n",
    "- Early Stopping: If early stopping is used, the exact epoch at which training stops can vary slightly between runs, leading to different model weights and performance.\n",
    "\n",
    "    To reduce these discrepancies, you can set random seeds for reproducibility. This can help ensure that the weight initialization and data shuffling are consistent across different runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee787f-35ae-4793-95bb-6dd07cbdb798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
