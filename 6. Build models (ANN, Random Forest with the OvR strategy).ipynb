{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b589229-e81c-413a-8bed-59fba1bed7f8",
   "metadata": {},
   "source": [
    "# Content\n",
    "- ANN Model Implementation for Classification\n",
    "    - Conclusion from the Results\n",
    "        - Overview of Metrics\n",
    "        - Training Set Classification Report\n",
    "        - Test Set Classification Report\n",
    "    - Key Insights\n",
    "    - Recommendations\n",
    "- Random Forest with the OvR strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ad9bc-c158-483d-9717-68d85e115347",
   "metadata": {},
   "source": [
    "## ANN Model Implementation for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667509-7f9e-4092-a12c-d5669b9ce291",
   "metadata": {},
   "source": [
    "To predict the target columns (Segmentation_A, Segmentation_B, Segmentation_C, Segmentation_D) using an Artificial Neural Network (ANN) for a multi-class classification task, we can utilize libraries like TensorFlow and Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6e734ec-7892-45c4-9b32-9cf18e2ae5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.3924 - loss: 1.2842 - val_accuracy: 0.4730 - val_loss: 1.1736\n",
      "Epoch 2/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4788 - loss: 1.1628 - val_accuracy: 0.4902 - val_loss: 1.1359\n",
      "Epoch 3/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4972 - loss: 1.1239 - val_accuracy: 0.5041 - val_loss: 1.1181\n",
      "Epoch 4/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5146 - loss: 1.1016 - val_accuracy: 0.5090 - val_loss: 1.1085\n",
      "Epoch 5/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5219 - loss: 1.0864 - val_accuracy: 0.5114 - val_loss: 1.1037\n",
      "Epoch 6/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5307 - loss: 1.0743 - val_accuracy: 0.5123 - val_loss: 1.1005\n",
      "Epoch 7/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5374 - loss: 1.0648 - val_accuracy: 0.5139 - val_loss: 1.0991\n",
      "Epoch 8/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5418 - loss: 1.0569 - val_accuracy: 0.5123 - val_loss: 1.0977\n",
      "Epoch 9/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5507 - loss: 1.0500 - val_accuracy: 0.5147 - val_loss: 1.0967\n",
      "Epoch 10/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5541 - loss: 1.0440 - val_accuracy: 0.5163 - val_loss: 1.0968\n",
      "Epoch 11/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5562 - loss: 1.0389 - val_accuracy: 0.5196 - val_loss: 1.0955\n",
      "Epoch 12/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5574 - loss: 1.0339 - val_accuracy: 0.5155 - val_loss: 1.0962\n",
      "Epoch 13/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5586 - loss: 1.0290 - val_accuracy: 0.5204 - val_loss: 1.0963\n",
      "Epoch 14/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5618 - loss: 1.0246 - val_accuracy: 0.5204 - val_loss: 1.0972\n",
      "Epoch 15/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5659 - loss: 1.0203 - val_accuracy: 0.5180 - val_loss: 1.0986\n",
      "Epoch 16/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5683 - loss: 1.0165 - val_accuracy: 0.5172 - val_loss: 1.0993\n",
      "Epoch 17/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5718 - loss: 1.0123 - val_accuracy: 0.5139 - val_loss: 1.1006\n",
      "Epoch 18/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5721 - loss: 1.0087 - val_accuracy: 0.5139 - val_loss: 1.1016\n",
      "Epoch 19/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5720 - loss: 1.0048 - val_accuracy: 0.5163 - val_loss: 1.1030\n",
      "Epoch 20/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5762 - loss: 1.0012 - val_accuracy: 0.5131 - val_loss: 1.1049\n",
      "Epoch 21/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5747 - loss: 0.9976 - val_accuracy: 0.5147 - val_loss: 1.1059\n",
      "Epoch 22/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5785 - loss: 0.9941 - val_accuracy: 0.5139 - val_loss: 1.1081\n",
      "Epoch 23/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5741 - loss: 0.9908 - val_accuracy: 0.5163 - val_loss: 1.1097\n",
      "Epoch 24/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5777 - loss: 0.9875 - val_accuracy: 0.5172 - val_loss: 1.1110\n",
      "Epoch 25/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5773 - loss: 0.9842 - val_accuracy: 0.5196 - val_loss: 1.1133\n",
      "Epoch 26/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5810 - loss: 0.9809 - val_accuracy: 0.5180 - val_loss: 1.1147\n",
      "Epoch 27/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5825 - loss: 0.9773 - val_accuracy: 0.5163 - val_loss: 1.1169\n",
      "Epoch 28/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5854 - loss: 0.9741 - val_accuracy: 0.5163 - val_loss: 1.1182\n",
      "Epoch 29/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5871 - loss: 0.9708 - val_accuracy: 0.5163 - val_loss: 1.1202\n",
      "Epoch 30/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5878 - loss: 0.9678 - val_accuracy: 0.5147 - val_loss: 1.1220\n",
      "Epoch 31/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5907 - loss: 0.9643 - val_accuracy: 0.5123 - val_loss: 1.1245\n",
      "Epoch 32/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5896 - loss: 0.9614 - val_accuracy: 0.5180 - val_loss: 1.1274\n",
      "Epoch 33/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5916 - loss: 0.9586 - val_accuracy: 0.5172 - val_loss: 1.1298\n",
      "Epoch 34/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5901 - loss: 0.9553 - val_accuracy: 0.5163 - val_loss: 1.1317\n",
      "Epoch 35/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5935 - loss: 0.9522 - val_accuracy: 0.5172 - val_loss: 1.1337\n",
      "Epoch 36/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5967 - loss: 0.9489 - val_accuracy: 0.5147 - val_loss: 1.1365\n",
      "Epoch 37/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5992 - loss: 0.9460 - val_accuracy: 0.5155 - val_loss: 1.1385\n",
      "Epoch 38/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6011 - loss: 0.9432 - val_accuracy: 0.5114 - val_loss: 1.1414\n",
      "Epoch 39/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6045 - loss: 0.9399 - val_accuracy: 0.5098 - val_loss: 1.1443\n",
      "Epoch 40/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6060 - loss: 0.9372 - val_accuracy: 0.5123 - val_loss: 1.1462\n",
      "Epoch 41/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6049 - loss: 0.9343 - val_accuracy: 0.5114 - val_loss: 1.1488\n",
      "Epoch 42/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6057 - loss: 0.9313 - val_accuracy: 0.5114 - val_loss: 1.1513\n",
      "Epoch 43/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6054 - loss: 0.9285 - val_accuracy: 0.5090 - val_loss: 1.1533\n",
      "Epoch 44/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6103 - loss: 0.9256 - val_accuracy: 0.5041 - val_loss: 1.1561\n",
      "Epoch 45/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6119 - loss: 0.9229 - val_accuracy: 0.5082 - val_loss: 1.1578\n",
      "Epoch 46/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6136 - loss: 0.9204 - val_accuracy: 0.5065 - val_loss: 1.1621\n",
      "Epoch 47/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6149 - loss: 0.9177 - val_accuracy: 0.5033 - val_loss: 1.1637\n",
      "Epoch 48/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6138 - loss: 0.9149 - val_accuracy: 0.5000 - val_loss: 1.1660\n",
      "Epoch 49/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6142 - loss: 0.9128 - val_accuracy: 0.5008 - val_loss: 1.1700\n",
      "Epoch 50/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6137 - loss: 0.9105 - val_accuracy: 0.5008 - val_loss: 1.1721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1db751101d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "import tensorflow as tf\n",
    "import random\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train_split_dimentionality_reducted.csv')\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df.drop(['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D'], axis=1)\n",
    "y = df[['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the ANN model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')  # 4 output nodes for the 4 target classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "719a1c55-26a6-49dc-acff-6082b90e0ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6050 - loss: 0.9096\n",
      "\n",
      "Train loss: 0.8917937278747559\n",
      "\n",
      "Train accuracy: 0.616830050945282\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4813 - loss: 1.1781 \n",
      "\n",
      "Test loss: 1.1720653772354126\n",
      "\n",
      "Test accuracy: 0.5008170008659363\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.58      0.54      0.56      1230\n",
      "Segmentation_B       0.55      0.47      0.51      1160\n",
      "Segmentation_C       0.65      0.59      0.62      1159\n",
      "Segmentation_D       0.66      0.84      0.74      1347\n",
      "\n",
      "      accuracy                           0.62      4896\n",
      "     macro avg       0.61      0.61      0.61      4896\n",
      "  weighted avg       0.61      0.62      0.61      4896\n",
      "\n",
      "\n",
      "Test Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.42      0.38      0.40       309\n",
      "Segmentation_B       0.39      0.34      0.36       283\n",
      "Segmentation_C       0.59      0.53      0.56       292\n",
      "Segmentation_D       0.56      0.72      0.63       340\n",
      "\n",
      "      accuracy                           0.50      1224\n",
      "     macro avg       0.49      0.49      0.49      1224\n",
      "  weighted avg       0.49      0.50      0.49      1224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print('\\nTrain loss:', train_loss)\n",
    "print('\\nTrain accuracy:', train_accuracy)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print('\\nTest loss:', test_loss)\n",
    "print('\\nTest accuracy:', test_accuracy)\n",
    "\n",
    "# Predict the classes for training data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n",
    "y_train_labels = np.argmax(np.array(y_train), axis=1)\n",
    "\n",
    "# Predict the classes for test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "y_test_labels = np.argmax(np.array(y_test), axis=1)\n",
    "\n",
    "# Print the classification report for the training set\n",
    "print(\"\\nTraining Set Classification Report\")\n",
    "print(classification_report(y_train_labels, y_train_pred_labels, target_names=target_names))\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"\\nTest Set Classification Report\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7c024-5fae-4765-bb21-84a4ebf6aa7b",
   "metadata": {},
   "source": [
    "## Conclusion from the Results\n",
    "\n",
    "### Overview of Metrics\n",
    "- **Training Loss**: 0.8782\n",
    "- **Training Accuracy**: 0.6242\n",
    "- **Test Loss**: 1.1511\n",
    "- **Test Accuracy**: 0.5172\n",
    "\n",
    "### Training Set Classification Report\n",
    "- **Overall Accuracy**: 0.62\n",
    "- **Precision, Recall, F1-Score**:\n",
    "  - Segmentation_A: Precision (0.57), Recall (0.63), F1-Score (0.60)\n",
    "  - Segmentation_B: Precision (0.58), Recall (0.40), F1-Score (0.47)\n",
    "  - Segmentation_C: Precision (0.62), Recall (0.66), F1-Score (0.64)\n",
    "  - Segmentation_D: Precision (0.70), Recall (0.78), F1-Score (0.74)\n",
    "\n",
    "### Test Set Classification Report\n",
    "- **Overall Accuracy**: 0.52\n",
    "- **Precision, Recall, F1-Score**:\n",
    "  - Segmentation_A: Precision (0.41), Recall (0.46), F1-Score (0.43)\n",
    "  - Segmentation_B: Precision (0.45), Recall (0.33), F1-Score (0.38)\n",
    "  - Segmentation_C: Precision (0.59), Recall (0.60), F1-Score (0.59)\n",
    "  - Segmentation_D: Precision (0.60), Recall (0.66), F1-Score (0.63)\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Overfitting**: The training accuracy (0.6242) is higher than the test accuracy (0.5172), and the training loss (0.8782) is lower than the test loss (1.1511). This suggests that the model may be overfitting to the training data, capturing noise and patterns that do not generalize well to the test data.\n",
    "\n",
    "2. **Class Imbalance Impact**:\n",
    "    - **Segmentation_D** has the highest precision, recall, and F1-score in both the training and test sets, suggesting that the model is better at predicting this class.\n",
    "    - **Segmentation_B** has the lowest recall and F1-score in both sets, indicating that the model struggles to correctly identify instances of this class.\n",
    "\n",
    "3. **Model Performance**:\n",
    "    - **Training Performance**: The model has a decent performance on the training set with an accuracy of 62%. The F1-scores for the different classes range from 0.47 to 0.74.\n",
    "    - **Test Performance**: The model's performance drops on the test set with an accuracy of 51.7%. The F1-scores range from 0.38 to 0.63, indicating variability in the model's ability to generalize across different segments.\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Address Overfitting**:\n",
    "   - **Regularization**: Implement regularization techniques such as dropout, L1/L2 regularization to prevent overfitting.\n",
    "   - **Early Stopping**: Use early stopping during training to halt training once the model's performance on a validation set stops improving.\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Perform hyperparameter tuning using techniques such as GridSearchCV or RandomizedSearchCV to find the optimal set of hyperparameters for the model.\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - Consider experimenting with more complex models or architectures if the current model is too simple.\n",
    "   - Alternatively, if the model is too complex, try simplifying the architecture.\n",
    "\n",
    "4. **Cross-Validation**:\n",
    "   - Implement cross-validation to ensure that the model's performance is consistent across different subsets of the data.\n",
    "\n",
    "By addressing these areas, you can improve the model's performance and its ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8302d7-c14b-46cd-b01e-7c3e25c14366",
   "metadata": {},
   "source": [
    "## Random Forest with the OvR strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "436862ba-18d1-49d0-8662-3559fcf83b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (6120, 20)\n",
      "Target shape: (6120, 4)\n",
      "X_train shape: (4896, 20), y_train shape: (4896, 4)\n",
      "X_test shape: (1224, 20), y_test shape: (1224, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, log_loss, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train_split_dimentionality_reducted.csv')\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df.drop(['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D'], axis=1)\n",
    "y = df[['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']]\n",
    "\n",
    "# Debug: Check shapes of X and y\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Debug: Check shapes of train and test sets\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Build and train the RandomForest model with OvR strategy\n",
    "model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate train and test accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate train and test log loss\n",
    "train_loss = log_loss(y_train, model.predict_proba(X_train))\n",
    "test_loss = log_loss(y_test, model.predict_proba(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7781cbb-7e80-40b7-af5a-44600fe56113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9483251633986928\n",
      "Test Accuracy: 0.34477124183006536\n",
      "Train Loss: 0.28930229061852214\n",
      "Test Loss: 1.6616638840644908\n",
      "Training Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.98      0.95      0.97      1230\n",
      "Segmentation_B       0.98      0.92      0.95      1160\n",
      "Segmentation_C       0.97      0.93      0.95      1159\n",
      "Segmentation_D       0.99      0.98      0.98      1347\n",
      "\n",
      "     micro avg       0.98      0.95      0.96      4896\n",
      "     macro avg       0.98      0.95      0.96      4896\n",
      "  weighted avg       0.98      0.95      0.96      4896\n",
      "   samples avg       0.95      0.95      0.95      4896\n",
      "\n",
      "Test Set Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Segmentation_A       0.42      0.26      0.32       309\n",
      "Segmentation_B       0.37      0.20      0.26       283\n",
      "Segmentation_C       0.53      0.37      0.43       292\n",
      "Segmentation_D       0.67      0.55      0.60       340\n",
      "\n",
      "     micro avg       0.52      0.35      0.42      1224\n",
      "     macro avg       0.49      0.34      0.40      1224\n",
      "  weighted avg       0.50      0.35      0.41      1224\n",
      "   samples avg       0.35      0.35      0.35      1224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print train and test accuracy and loss\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Train Loss: {train_loss}\")\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Print the classification report for the training set\n",
    "print(\"Training Set Classification Report\")\n",
    "print(classification_report(y_train, y_train_pred, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Test Set Classification Report\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Segmentation_A', 'Segmentation_B', 'Segmentation_C', 'Segmentation_D']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4db14-e27a-4de5-8c8c-442dd3514c64",
   "metadata": {},
   "source": [
    "The observed issue of very high train accuracy and relatively low test accuracy indicates overfitting, meaning the model performs well on the training data but poorly on the unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4d728-c55f-4e02-bfbf-86d7ac71f112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
